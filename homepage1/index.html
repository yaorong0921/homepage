<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
 
  <title>Yao Rong</title>
  
  <meta name="author" content="Yao Rong>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yao Rong</name>
              </p>
              <p align="center">&nbsp;</p>
              <p>I am a Ph.D. student in the group of <a href="https://www.edu.sot.tum.de/en/hctl/home/">Human-Centered Technologies for Learning</a> led by <a href="https://www.edu.sot.tum.de/en/hctl/prof-dr-enkelejda-kasneci/">Prof. Dr. Enkelejda Kasneci</a> at <strong>Technical University of Munich</strong>. I received my master's degree in Electrical and Computer Engineering from TU Munich in 2019, and B.Eng. degrees in Mechatronics from both Tongji University in Shanghai and Munich University of Applied Sciences in 2016, as part of a double degree program.
              </p> 
              <p> My research interest lies at the intersection of AI systems and human interaction with a specific focus on human-centered Explainable AI (XAI), human-centered AI, and deep learning. I am particularly focusing on how XAI techniques can enhance the understanding, trust, and usability of AI systems for end-users. By incorporating user feedback, human cognitive models, and effective model explanation techniques, my aim is to bridge the gap between complex AI algorithms and human interpretability, ultimately fostering responsible and transparent AI deployment in various domains.
              </p>
              <p style="text-align:center">
                <a href="mailto:yao.rong@tum.de">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2CGNfAEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yaorong0921">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/yaorong0921">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yao-rong-7b55a8255/">LinkedIn</a> &nbsp/&nbsp
                <a href="data/yaorong_cv_2023.pdf">CV</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:33%;max-width:33%">
              <a href="images/yao_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yao_profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <p>
          <name>News</name>
        </p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr><td>
          <ul>
          <li><nobr> <strong>April 2023</strong>: I move to Munich and continue my Ph.D. at <strong>TU Munich</strong> in the group <a href="https://www.edu.sot.tum.de/en/hctl/home/">Human-Centered Technologies for Learning</a>.  </nobr></li>
          <li><nobr> <strong>September 2022</strong>: I join <strong>DATA Lab</strong> led by <a href="https://cs.rice.edu/~xh37/index.html">Dr. Xia "Ben" Hu</a> as a visiting scholar at <strong>Rice University, Houston</strong>.   </nobr></li> 
          <li><nobr> <strong>August 2022</strong>: I serve as a <strong>Diversity and Inclusion Chair</strong> at <a href="https://etra.acm.org/2023/"> the ACM Symposium on Eye Tracking Research and Applications (ETRA) 2023 </a>. </nobr></li>
          </ul>
          </td></tr>  
          </table> 



        <p>
          <name>Publications</name>
        </p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lara.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.05760" id="gnn">
                <papertitle>Efficient GNN Explanation via Learning Removal-based Attribution</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Guanchu Wang, Qizhang Feng, Ninghao Liu, Zirui Liu, Enkelejda Kasneci, Xia Hu
              <br>
              <em>preprint</em>, 2023
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/abs/2306.05760">[Paper]</a> 
                <a href=" https://anonymous.4open.science/r/LARA-10D8/README.md">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acmmm.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="" id="uai">
                <papertitle>Gaze-Guided Graph Neural Network for Action Anticipation Conditioned on Intention</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Süleyman Özdel, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang, Enkelejda Kasneci
              <br>
              <em> under review</em>, 2023
              <br>
              <div class="paper" id="pointrend">
                <a href="">[Paper]</a> 
                <a href="">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>          



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/uai.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2206.13872.pdf" id="uai">
                <papertitle>When are Post-hoc Conceptual Explanations Identifiable?</papertitle>
              </a>
              <br>
              Tobias Leemann*, Michael Kirchhof*, <strong>Yao Rong</strong>, Enkelejda Kasneci, Gjergji Kasneci
              <br>
              <em> UAI</em>, 2023
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/abs/2206.13872">[Paper]</a> 
                <a href="https://github.com/tleemann/identifiable_concepts">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dynstaf.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Rong_DynStatF_An_Efficient_Feature_Fusion_Strategy_for_LiDAR_3D_Object_CVPRW_2023_paper.pdf" id="cvprw">
                <papertitle>DynStatF: An Efficient Feature Fusion Strategy for LiDAR 3D Object Detection</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>*, Xiangyu Wei*, Tianwei Lin, Yueyu Wang, Enkelejda Kasneci
              <br>
              <em> End-to-End Autonomous Driving @ CVPR</em>, 2023
              <br>
              <div class="paper" id="pointrend">
                <a href="https://openaccess.thecvf.com/content/CVPR2023W/E2EAD/papers/Rong_DynStatF_An_Efficient_Feature_Fusion_Strategy_for_LiDAR_3D_Object_CVPRW_2023_paper.pdf">[Paper]</a> 
                <a href="">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hxai_survey.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.11584" id="hxai_survey">
                <papertitle>Towards Human-centered Explainable AI: User Studies for Model Explanations</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Tobias Leemann, Thai-trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel, Gjergji Kasneci, Enkelejda Kasneci
              <br>
              <em>arXiv Preprint</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/abs/2210.11584">[Paper]</a> 
                <a href="https://github.com/yaorong0921/hxai-survey">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/road_preprint.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2202.00449.pdf" id="road_preprint">
                <papertitle>Evaluating Feature Attribution: An Information-Theoretic Perspective</papertitle>
              </a>
              <br>
              <strong>Yao Rong*</strong>, Tobias Leemann*, Vadim Borisov, Gjergji Kasneci, Enkelejda Kasneci
              <br>
              <em>ICML</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2202.00449.pdf">[Paper]</a> 
                <a href="https://github.com/tleemann/road_evaluation">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/driver_yolo5.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.12150.pdf" id="pacmhci">
                <papertitle>Where and What: Driver Attention-based Object Detection</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Naemi-Rebecca Kassautzki, Wolfgang Fuhl, Enkelejda Kasneci
              <br>
              <em>PACMHCI</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2204.12150.pdf">[Paper]</a>
                <a href=https://github.com/yaorong0921/driver-gaze-yolov5>[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/user_trust.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.12230.pdf" id="chiws">
                <papertitle>User Trust on an Explainable AI-based Medical Diagnosis Support System</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Nora Castner, Efe Bozkir, Enkelejda Kasneci
              <br>
              <em>TRAIT at CHI</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2204.12230.pdf">[Paper]</a>
              </div>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ha_bmvc.png" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.01628.pdf" id="HA">
                <papertitle>Human Attention in Fine-grained Classification</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Wenjia Xu, Zeynep Akata, Enkelejda Kasneci
              <br>
              <em>BMVC</em>, 2021
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2111.01628.pdf">[Paper]</a>
                <a href="https://github.com/yaorong0921/CUB-GHA">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/itsm.png" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.02082.pdf" id="itsm">
                <papertitle>Artificial Intelligence Methods in In-Cabin Use Cases: A Survey</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Chao Han, Christian Hellert, Antje Loyal, Enkelejda Kasneci
              <br>
              <em>ITSM</em>, 2021
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2101.02082.pdf">[Paper]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/itsc_demo.gif" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.11557.pdf" id="itsc">
                <papertitle>Driver Intention Anticipation Based on In-Cabin and Driving Scene Monitoring</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Zeynep Akata, Enkelejda Kasneci
              <br>
              <em>ITSC</em>, 2020
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2006.11557.pdf">[Paper]</a> 
                <a href="https://github.com/yaorong0921/Driver-Intention-Prediction">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DMHG.jpg" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.00951.pdf" id="DMHG">
                <papertitle> DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework</papertitle>
              </a>
              <br>
              Okan Köpüklü, Thomas Ledwon, <strong>Yao Rong</strong>, Neslihan Kose, Gerhard Rigoll
              <br>
              <em>FG</em>, 2020
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2003.00951.pdf">[Paper]</a> 
                <a href="https://www.ei.tum.de/mmk/DriverMHG/">[Dataset]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccvw_demo.gif" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.pdf" id="DMHG">
                <papertitle> Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs</papertitle>
              </a>
              <br>
              Okan Köpüklü, <strong>Yao Rong</strong>, Gerhard Rigoll
              <br>
              <em>ICCV Workshop</em>, 2019
              <br>
              <div class="paper" id="pointrend">
                <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.pdf">[Paper]</a> 
                <a href="https://www.ei.tum.de/mmk/shgd/">[Dataset]</a>
                <a href="https://github.com/yaorong0921/GeScale">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>


        </tbody></table>
        <p align="center">&nbsp;</p>
        <p align="center">&nbsp;</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <div style="clear:both;">
                <p align="right"><font size="2"><a href="http://jonbarron.info">Link of the template</a></font></p><br />
              </div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>
