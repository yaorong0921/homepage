<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
 
  <title>Yao Rong</title>
  
  <meta name="author" content="Yao Rong>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yao Rong</name>
              </p>
              <p align="center">&nbsp;</p>
              <p>I am a PhD studnet in <a href="https://www.hci.uni-tuebingen.de/chair/">Human Computer Interaction</a> led by <a href="https://www.hci.uni-tuebingen.de/chair/team/">Prof. Enkelejda Kasneci</a> at University of Tübingen. I recieved my master's degree in Electrical and Computer Engineering from TU Munich in 2019, and B.Eng. degrees in Mechatronics from both Tongji University in Shanghai and Munich University of Applied Sciences in 2016, as part of a double degree program.
              </p> 
              <p> My research interest lies at the intersection of human and artificial intelligence: Computer Vision, Explainable AI, Human-Computer Interaction, Human-centered AI and Deep Learning. I aim to bring human knowledge into AI models, thus developing AI applications that humans can well understand and trust.
              </p>
              <p style="text-align:center">
                <a href="mailto:yao.rong@uni-tuebingen.de">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2CGNfAEAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yaorong0921">Github</a> &nbsp/&nbsp
                <a href="data/yaorong_cv.pdf">CV</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:33%;max-width:33%">
              <a href="images/yao_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yao_profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <p>
          <name>Publications</name>
        </p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/driver_yolo5.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.12150.pdf" id="pacmhci">
                <papertitle>Where and What: Driver Attention-based Object Detection</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Naemi-Rebecca Kassautzki, Wolfgang Fuhl, Enkelejda Kasneci
              <br>
              <em>PACMHCI</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2204.12150.pdf">[Paper]</a>
                <a href=https://github.com/yaorong0921/driver-gaze-yolov5>[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/user_trust.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2204.12230.pdf" id="chiws">
                <papertitle>User Trust on an Explainable AI-based Medical Diagnosis Support System</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Nora Castner, Efe Bozkir, Enkelejda Kasneci
              <br>
              <em>TRAIT at CHI</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2204.12230.pdf">[Paper]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/road_preprint.png" alt="PontTuset" width="160" height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2202.00449.pdf" id="road_preprint">
                <papertitle>Evaluating Feature Attribution: An Information-Theoretic Perspective</papertitle>
              </a>
              <br>
              <strong>Yao Rong*</strong>, Tobias Leemann*, Vadim Borisov, Lorenzo Torresani, Gjergji Kasneci, Enkelejda Kasneci
              <br>
              <em>Pre-print</em>, 2022
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2202.00449.pdf">[Paper]</a> 
                <a href="https://github.com/tleemann/road_evaluation">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ha_bmvc.png" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.01628.pdf" id="HA">
                <papertitle>Human Attention in Fine-grained Classification</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Wenjia Xu, Zeynep Akata, Enkelejda Kasneci
              <br>
              <em>BMVC</em>, 2021
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2111.01628.pdf">[Paper]</a>
                <a href="https://github.com/yaorong0921/CUB-GHA">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/itsm.png" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.02082.pdf" id="itsm">
                <papertitle>Artificial Intelligence Methods in In-Cabin Use Cases: A Survey</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Chao Han, Christian Hellert, Antje Loyal, Enkelejda Kasneci
              <br>
              <em>ITSM</em>, 2021
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2101.02082.pdf">[Paper]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/itsc_demo.gif" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.11557.pdf" id="itsc">
                <papertitle>Driver Intention Anticipation Based on In-Cabin and Driving Scene Monitoring</papertitle>
              </a>
              <br>
              <strong>Yao Rong</strong>, Zeynep Akata, Enkelejda Kasneci
              <br>
              <em>ITSC</em>, 2020
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2006.11557.pdf">[Paper]</a> 
                <a href="https://github.com/yaorong0921/Driver-Intention-Prediction">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DMHG.jpg" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2003.00951.pdf" id="DMHG">
                <papertitle> DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework</papertitle>
              </a>
              <br>
              Okan Köpüklü, Thomas Ledwon, <strong>Yao Rong</strong>, Neslihan Kose, Gerhard Rigoll
              <br>
              <em>FG</em>, 2020
              <br>
              <div class="paper" id="pointrend">
                <a href="https://arxiv.org/pdf/2003.00951.pdf">[Paper]</a> 
                <a href="https://www.ei.tum.de/mmk/DriverMHG/">[Dataset]</a>
              </div>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccvw_demo.gif" alt="PontTuset" width="160"  height="130"  style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.pdf" id="DMHG">
                <papertitle> Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs</papertitle>
              </a>
              <br>
              Okan Köpüklü, <strong>Yao Rong</strong>, Gerhard Rigoll
              <br>
              <em>ICCV Workshop</em>, 2019
              <br>
              <div class="paper" id="pointrend">
                <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/Kopuklu_Talking_With_Your_Hands_Scaling_Hand_Gestures_and_Recognition_With_ICCVW_2019_paper.pdf">[Paper]</a> 
                <a href="https://www.ei.tum.de/mmk/shgd/">[Dataset]</a>
                <a href="https://github.com/yaorong0921/GeScale">[Code]</a>
              </div>
              <p></p>
            </td>
          </tr>


        </tbody></table>
        <p align="center">&nbsp;</p>
        <p align="center">&nbsp;</p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <div style="clear:both;">
                <p align="right"><font size="2"><a href="http://jonbarron.info">Link of the template</a></font></p><br />
              </div>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>
